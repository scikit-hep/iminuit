{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "negative-concord",
   "metadata": {},
   "source": [
    "# Cost functions\n",
    "\n",
    "We give an in-depth guide on how to use the builtin cost functions.\n",
    "\n",
    "The iminuit package comes with a couple of common cost functions that you can import from `iminuit.cost` for convenience. Of course, you can write your own cost functions to use with iminuit, but most of the cost function is always the same. What really varies is the statistical model which predicts the probability density as a function of the parameter values. This you still have to provide yourself and the iminuit package will not include machinery to build statistical models (that is out of scope).\n",
    "\n",
    "Using the builtin cost functions is not only convenient, they also have some extra features.\n",
    "\n",
    "* Support of fitted weighted histograms.\n",
    "* Technical tricks improve numerical stability.\n",
    "* Optional numba acceleration (if numba is installed).\n",
    "* Cost functions can be added to fit data sets with shared parameters.\n",
    "* Temporarily mask data.\n",
    "\n",
    "We demonstrate each cost function on an standard example from high-energy physics, the fit of a peak over some smooth background (here taken to be constant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lucky-canvas",
   "metadata": {},
   "outputs": [],
   "source": [
    "from iminuit import cost, Minuit\n",
    "from numba_stats import norm, uniform # faster replacements for scipy.stats functions\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absent-missile",
   "metadata": {},
   "source": [
    "We generate our data. We sample from a Gaussian peak around 0 with width 0.1 and from uniform background from -1 to 1. We then bin the original data. One can fit the original or the binned data. The latter is often much faster and if the binning is fine enough, there is no loss in precision as we will see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-fusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "xrange = -1, 1\n",
    "\n",
    "rng = np.random.default_rng(1)\n",
    "\n",
    "xdata = rng.normal(0, 0.1, size=400)\n",
    "xdata = np.append(xdata, rng.uniform(*xrange, size=1000))\n",
    "\n",
    "n, xe = np.histogram(xdata, bins=50, range=xrange)\n",
    "cx = 0.5 * (xe[1:] + xe[:-1])\n",
    "dx = np.diff(xe)\n",
    "\n",
    "plt.errorbar(cx, n, n ** 0.5, fmt=\"ok\")\n",
    "plt.plot(xdata, np.zeros_like(xdata), \"|\", alpha=0.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interesting-cursor",
   "metadata": {},
   "source": [
    "## Maximum-likelihood fits\n",
    "\n",
    "Maximum-likelihood fits are the state-of-the-art when it comes to fitting models to data. The can be applied to unbinned and binned data (histograms).\n",
    "\n",
    "* Unbinned fits are the easiest to use, because they can be apply directly to the raw sample. They become slow when the sample size is large.\n",
    "* Binned fits require you to appropriately bin the data. The binning has to be fine enough to retain all essential information. Binned fits are much faster when the sample size is large.\n",
    "\n",
    "### Unbinned fit\n",
    "\n",
    "Unbinned fits are ideal when the data samples are not too large or very high dimensional. There is no need to worry about the appropriate binning of the data. Unbinned fits are inefficient when the samples are very large and can become numerically unstable, too. Binned fits are a better choice then.\n",
    "\n",
    "The cost function for an unbinned maximum-likelihood fit is really simple, it is the sum of the logarithm of the pdf evaluated at each sample point (times -1 to turn maximimization into minimization). You can easily write this yourself, but a naive implementation will suffer from instabilities when the pdf becomes locally zero. Our implementation mitigates the instabilities to some extend.\n",
    "\n",
    "To perform the unbinned fit you need to provide the pdf of the model, which must be vectorized (a numpy ufunc). The pdf must be normalized, which means that the integral over the sample value range must be a constant for any combination of model parameters.\n",
    "\n",
    "The model pdf in this case is the weighted sum of the normal and the uniform pdfs. The parameters are $z$ (the weight), $\\mu$ and $\\sigma$ of the normal distribution. The uniform distribution is parameter-free. The cost function detects the parameter names.\n",
    "\n",
    "It is important to put appropriate limits on the parameters, so that the problem does not become mathematically undefined.\n",
    "* $0 < z < 1$,\n",
    "* $-1 < \\mu < 1$,\n",
    "* $\\sigma > 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-drama",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pdf(x, z, mu, sigma):\n",
    "    return (z * norm.pdf(x, mu, sigma) + \n",
    "            (1 - z) * uniform.pdf(x, xrange[0], xrange[1] - xrange[0]))\n",
    "\n",
    "c = cost.UnbinnedNLL(xdata, model_pdf)\n",
    "\n",
    "m = Minuit(c, z=0.4, mu=0, sigma=0.2)\n",
    "m.limits[\"z\"] = (0, 1)\n",
    "m.limits[\"mu\"] = (-1, 1)\n",
    "m.limits[\"sigma\"] = (0, None)\n",
    "\n",
    "m.migrad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaging-variable",
   "metadata": {},
   "source": [
    "We visualize the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-reservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(cx, n, n ** 0.5, fmt=\"ok\")\n",
    "xm = np.linspace(xe[0], xe[-1])\n",
    "plt.plot(xm, model_pdf(xm, *[p.value for p in m.init_params]) * len(xdata) * dx[0],\n",
    "         ls=\":\", label=\"init\")\n",
    "plt.plot(xm, model_pdf(xm, *m.values) * len(xdata) * dx[0], label=\"fit\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introductory-watershed",
   "metadata": {},
   "source": [
    "### Extended unbinned fit\n",
    "\n",
    "An important variant of the unbinned ML fit is described by [Roger Barlow, Nucl.Instrum.Meth.A 297 (1990) 496-506](https://inspirehep.net/literature/297773). Use this if both the shape and the integral of the density are of interest. In practice, this is often the case, for example, if you want to estimate a cross-section or yield.\n",
    "\n",
    "The model in this case has to return the integral of the density and the density itself (which must be vectorized). The parameters in this case are $n_\\mathrm{sig}$ (integral of the signal density), $n_\\mathrm{bkg}$ (integral of the uniform density), $\\mu$ and $\\sigma$ of the normal distribution. Again, the parameters need limits so that the problem is mathematically defined.\n",
    "\n",
    "* $n_\\mathrm{sig} > 0$,\n",
    "* $n_\\mathrm{bkg} > 0$,\n",
    "* $-1 < \\mu < 1$,\n",
    "* $\\sigma > 0$.\n",
    "\n",
    "Compared to the previous case, we have one more parameter to fit. This is common to extended fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expanded-japanese",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_density(x, nsig, nbkg, mu, sigma):    \n",
    "    return nsig + nbkg, (nsig * norm.pdf(x, mu, sigma) + \n",
    "        nbkg * uniform.pdf(x, xrange[0], xrange[1] - xrange[0]))\n",
    "\n",
    "c = cost.ExtendedUnbinnedNLL(xdata, model_density)\n",
    "\n",
    "m = Minuit(c, nsig=300, nbkg=1500, mu=0, sigma=0.2)\n",
    "m.limits[\"nsig\", \"nbkg\", \"sigma\"] = (0, None)\n",
    "m.limits[\"mu\"] = (-1, 1)\n",
    "\n",
    "m.migrad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understood-monte",
   "metadata": {},
   "source": [
    "The fitted values and the uncertainty estimates for $\\mu$ and $\\sigma$ are identical to the ordinary ML fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governmental-hardware",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(cx, n, n ** 0.5, fmt=\"ok\")\n",
    "xm = np.linspace(xe[0], xe[-1])\n",
    "plt.plot(xm, model_density(xm, *[p.value for p in m.init_params])[1] * dx[0],\n",
    "         ls=\":\", label=\"init\")\n",
    "plt.plot(xm, model_density(xm, *m.values)[1] * dx[0], label=\"fit\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlling-celebration",
   "metadata": {},
   "source": [
    "### Binned Fit\n",
    "\n",
    "Binned fits are computationally more efficient and numerically more stable when samples are large. The caveat is that one has to choose an appropriate binning. The binning should be fine enough so that the essential information in the original is retained.\n",
    "\n",
    "The sample investigate here is large enough and 50 bins are fine enough to retain all information. The maximum-likelihood method applied to binned data gives correct results even if bins no entries, so chosing a binning that is very fine is not an issue. It just increases the computational cost.\n",
    "\n",
    "The cost functions for binned fits implemented here assume that the bin contents are independently Poisson distributed around an unknown expected value per bin. This is exactly correct for ordinary histograms.\n",
    "\n",
    "For a binned maximum-likelihood fit, one sums the logarithm of Poisson probabilities for the observed counts as a function of the predicted counts in this case (times -1 to turn maximization into minimization). Instead of a pdf, you need to provide a cdf in this case (which must be vectorized). Note that you can approximate the cdf as \"bin-width times pdf evaluated at center\" if it is difficult to calculate, but this is an approxmiation. Using the cdf is exact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "robust-groove",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_cdf(xe, z, mu, sigma):\n",
    "    return (z * norm.cdf(xe, mu, sigma) + \n",
    "            (1-z) * uniform.cdf(xe, xrange[0], xrange[1] - xrange[0]))\n",
    "\n",
    "c = cost.BinnedNLL(n, xe, model_cdf)\n",
    "\n",
    "m = Minuit(c, z=0.4, mu=0, sigma=0.2)\n",
    "\n",
    "m.limits[\"z\"] = (0, 1)\n",
    "m.limits[\"mu\"] = (-1, 1)\n",
    "m.limits[\"sigma\"] = (0, None)\n",
    "\n",
    "m.migrad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparable-special",
   "metadata": {},
   "source": [
    "The fitted values and the uncertainty estimates for $\\mu$ and $\\sigma$ are not identical to the unbinned fit, but very close. For practical purposes, the results are equivalent. This shows that the binning is fine enough to retain the essential information in the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simplified-police",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(cx, n, n ** 0.5, fmt=\"ok\")\n",
    "plt.stairs(np.diff(model_cdf(xe, *[p.value for p in m.init_params])) * len(xdata), xe,\n",
    "           ls=\":\", label=\"init\")\n",
    "plt.stairs(np.diff(model_cdf(xe, *m.values)) * len(xdata), xe, label=\"fit\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decent-treat",
   "metadata": {},
   "source": [
    "### Extended binned maximum-likelihood fit\n",
    "\n",
    "The binned extended maximum-likelihood fit is strictly the binned equivalent of the corresponding unbinned fit. One sums the logarithm of Poisson probabilities for the observed counts as a function of the predicted counts in this case (times -1 to turn maximization into minimization).\n",
    "\n",
    "Instead of a density, you need to provide a cdf of the density in this case (which must be vectorized). There is no need to separately return the total integral like the unbinned case. The parameters are the same as in the unbinned extended fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suitable-fetish",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_density_cdf(xe, nsig, nbkg, mu, sigma):\n",
    "    return (nsig * norm.cdf(xe, mu, sigma) +\n",
    "            nbkg * uniform.cdf(xe, xrange[0], xrange[1] - xrange[0]))\n",
    "\n",
    "c = cost.ExtendedBinnedNLL(n, xe, model_density_cdf)\n",
    "\n",
    "m = Minuit(c, nsig=300, nbkg=1500, mu=0, sigma=0.2)\n",
    "\n",
    "m.limits[\"nsig\", \"nbkg\", \"sigma\"] = (0, None)\n",
    "m.limits[\"mu\"] = (-1, 1)\n",
    "\n",
    "m.migrad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collectible-amendment",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(cx, n, n ** 0.5, fmt=\"ok\")\n",
    "plt.stairs(np.diff(model_density_cdf(xe, *[p.value for p in m.init_params])), xe,\n",
    "           ls=\":\", label=\"init\")\n",
    "plt.stairs(np.diff(model_density_cdf(xe, *m.values)), xe, label=\"fit\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noticed-wireless",
   "metadata": {},
   "source": [
    "The fitted values and the uncertainty estimates for $\\mu$ and $\\sigma$ are identical to the ordinary ML fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infectious-trash",
   "metadata": {},
   "source": [
    "### Temporary masking\n",
    "\n",
    "In complicated binned fits with peak and background, it is sometimes useful to fit in several stages. One typically starts by masking the signal region, to fit only the background region.\n",
    "\n",
    "The cost functions have a mask attribute to that end. We demonstrate the use of the mask with an extended binned fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ruled-society",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_density_cdf(xe, nsig, nbkg, mu, sigma):\n",
    "    return (nsig * norm.cdf(xe, mu, sigma) +\n",
    "            nbkg * uniform.cdf(xe, xrange[0], xrange[1] - xrange[0]))\n",
    "\n",
    "c = cost.ExtendedBinnedNLL(n, xe, model_density_cdf)\n",
    "\n",
    "# we set the signal amplitude to zero and fix all signal parameters\n",
    "m = Minuit(c, nsig=300, nbkg=1500, mu=0, sigma=0.2)\n",
    "\n",
    "m.limits[\"nsig\", \"nbkg\", \"sigma\"] = (0, None)\n",
    "m.limits[\"mu\"] = (-1, 1)\n",
    "m.fixed[\"nsig\", \"mu\", \"sigma\"] = True\n",
    "\n",
    "# we temporarily mask out the signal\n",
    "c.mask = (cx < -0.5) | (0.5 < cx)\n",
    "\n",
    "m.migrad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "happy-diabetes",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(cx, n, n ** 0.5, fmt=\"ok\")\n",
    "plt.stairs(np.diff(model_density_cdf(xe, *[p.value for p in m.init_params])), xe,\n",
    "           ls=\":\", label=\"init\")\n",
    "plt.stairs(np.diff(model_density_cdf(xe, *m.values)), xe, label=\"fit\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heard-jurisdiction",
   "metadata": {},
   "source": [
    "Now we fix the background and fit only the signal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accredited-dispute",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.mask = None # remove mask\n",
    "m.fixed = False # release all parameters\n",
    "m.fixed[\"nbkg\"] = True # fix background amplitude\n",
    "\n",
    "m.migrad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "timely-afternoon",
   "metadata": {},
   "source": [
    "Finally, we release all parameters and fit again to get the correct uncertainty estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recreational-pride",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.fixed = None\n",
    "m.migrad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wanted-population",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(cx, n, n ** 0.5, fmt=\"ok\")\n",
    "plt.stairs(np.diff(model_density_cdf(xe, *[p.value for p in m.init_params])), xe,\n",
    "           ls=\":\", label=\"init\")\n",
    "plt.stairs(np.diff(model_density_cdf(xe, *m.values)), xe, label=\"fit\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "correct-notice",
   "metadata": {},
   "source": [
    "We get the same result, of course. Since this was an easy problem, we did not need these extra steps, but doing this is usually helpful to fit lots of histograms without adjusting each fit manually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tough-europe",
   "metadata": {},
   "source": [
    "### Weighted histograms\n",
    "\n",
    "The cost functions for binned data also support weighted histograms. Just pass an array with the shape `(n, 2)` instead of `(n,)` as the first argument, where the first number of each pair is the sum of weights and the second is the sum of weights squared (an estimate of the variance of that bin value)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gothic-regular",
   "metadata": {},
   "source": [
    "## Least-squares fits\n",
    "\n",
    "A cost function for a general weighted least-squares fit (aka chi-square fit) is also included. In statistics this is called non-linear regression.\n",
    "\n",
    "In this case you need to provide a model that predicts the y-values as a function of the x-values and the parameters. The fit needs estimates of the y-errors. If those are wrong, the fit may be biased. If your data has errors on the x-values as well, checkout the tutorial about automatic differentiation, which includes an application of that to such fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "packed-penguin",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x, a, b):\n",
    "    return a + b * x ** 2\n",
    "\n",
    "rng = np.random.default_rng(4)\n",
    "\n",
    "x = np.linspace(0, 1, 20)\n",
    "yt = model(x, 1, 2)\n",
    "ye = 0.4 * x**5 + 0.1\n",
    "y = rng.normal(yt, ye)\n",
    "\n",
    "plt.plot(x, yt, ls=\"--\", label=\"truth\")\n",
    "plt.errorbar(x, y, ye, fmt=\"ok\", label=\"data\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-plant",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = cost.LeastSquares(x, y, ye, model)\n",
    "\n",
    "m = Minuit(c, a=0, b=0)\n",
    "m.migrad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "former-dominant",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(x, y, ye, fmt=\"ok\", label=\"data\")\n",
    "plt.plot(x, model(x, 1, 2), ls=\"--\", label=\"truth\")\n",
    "plt.plot(x, model(x, *m.values), label=\"fit\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-howard",
   "metadata": {},
   "source": [
    "### Robust least-squares\n",
    "\n",
    "The builtin least-squares function also supports robust fitting with alternative loss functions. See the documentation of `iminuit.cost.LeastSquares` for details. Users can pass their own loss functions. Builtin loss functions are:\n",
    "\n",
    "* `linear` (default): gives ordinary weighted least-squares\n",
    "* `soft_l1`: quadratic ordinary loss for small deviations ($\\ll 1\\sigma$), linear loss for large deviations ($\\gg 1\\sigma$), and smooth interpolation in between\n",
    "\n",
    "Let's create one outlier and see what happens with ordinary loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-singles",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x, a, b):\n",
    "    return a + b * x ** 2\n",
    "\n",
    "rng = np.random.default_rng(4)\n",
    "\n",
    "x = np.linspace(0, 1, 20)\n",
    "yt = model(x, 1, 2)\n",
    "ye = 0.4 * x**5 + 0.1\n",
    "y = rng.normal(yt, ye)\n",
    "y[3] = 3 # generate an outlier\n",
    "\n",
    "c = cost.LeastSquares(x, y, ye, model)\n",
    "\n",
    "m = Minuit(c, a=0, b=0)\n",
    "m.migrad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "available-organic",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(x, y, ye, fmt=\"ok\", label=\"data\")\n",
    "plt.plot(x, model(x, 1, 2), ls=\"--\", label=\"truth\")\n",
    "plt.plot(x, model(x, *m.values), label=\"fit\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpful-train",
   "metadata": {},
   "source": [
    "The result is distorted by the outlier. We can repair this with the `soft_l1` loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cheap-truth",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.loss = \"soft_l1\"\n",
    "m.migrad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulated-default",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(x, y, ye, fmt=\"ok\", label=\"data\")\n",
    "plt.plot(x, model(x, 1, 2), ls=\"--\", label=\"truth\")\n",
    "plt.plot(x, model(x, *m.values), label=\"fit\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attractive-porcelain",
   "metadata": {},
   "source": [
    "The result is now practically identical as in the previous case without an outlier.\n",
    "\n",
    "Robust fitting is a very powerful if the data are contaminated with small amounts of outliers. It comes with a price, however, the uncertainties are in general larger. Compare the estimated uncertainty of the parameter $b$, which was 0.15 and now is 0.23.\n",
    "\n",
    "We can actually do better by manually removing the point (using the mask attribute) and switching back to ordinary loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-wallet",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.mask = c.x != c.x[3]\n",
    "c.loss = \"linear\"\n",
    "m.migrad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varied-rhythm",
   "metadata": {},
   "source": [
    "Now the uncertainty on $b$ is back to 0.15."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
