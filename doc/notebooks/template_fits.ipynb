{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template fits\n",
    "\n",
    "In applications we are interested in separating a signal component from background components, we often fit parameteric models to data. Sometimes constructing a parametric model for some component is difficult. In that case, one fits a template instead which may be obtained from simulation or from a calibration sample in which a pure component can be isolated.\n",
    "\n",
    "The challenge then is to propagate the uncertainty of the template into the result. The template is now also estimated from a sample (be it simulated or a calibration sample), and the uncertainty associated to that can be substantial. We investigate different approaches for template fits, including the Barlow-Beeston and Barlow-Beeston-lite methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iminuit import Minuit\n",
    "from iminuit.cost import poisson_chi2, Template, ExtendedBinnedNLL\n",
    "import numpy as np\n",
    "from scipy.stats import norm, truncexpon\n",
    "from scipy.optimize import root_scalar, minimize\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from collections import defaultdict\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a toy example, we generate a mixture of two components: a normally distributed signal and exponentially distributed background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(rng, nmc, truth, bins):\n",
    "    xe = np.linspace(0, 2, bins + 1)\n",
    "    b = np.diff(truncexpon(1, 0, 2).cdf(xe))\n",
    "    s = np.diff(norm(1, 0.1).cdf(xe))\n",
    "    n = rng.poisson(b * truth[0]) + rng.poisson(s * truth[1])\n",
    "    t = np.array([rng.poisson(b * nmc), rng.poisson(s * nmc)])\n",
    "    return xe, n, t\n",
    "\n",
    "rng = np.random.default_rng(1)\n",
    "truth = 750, 250\n",
    "xe, n, t = generate(rng, 100, truth, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is visualized on the left-hand side. The templates are shown on the right-hand side. To show the effect of uncertainties in the template, this example intentially uses templates with poor statistical resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4), sharex=True)\n",
    "ax[0].stairs(n, xe, fill=True, color=\"k\", alpha=0.5, label=\"data\")\n",
    "for i, ti in enumerate(t):\n",
    "    ax[1].stairs(ti, xe, fill=True, alpha=0.5, label=f\"template {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapping template uncertainties\n",
    "\n",
    "Bootstrapping is a general purpose technique to include uncertainties backed up by bootstrap theory, so it can be applied to this problem.\n",
    "We perform a standard fit and pretend that the templates have no uncertainties. Then, we repeat this fit many times with templates that are fluctuated around the actual values assuming a Poisson distribution.\n",
    "\n",
    "There is no built-in cost function in iminuit for a template fit, so we write the cost function for this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(yields):\n",
    "    mu = 0\n",
    "    for y, c in zip(yields, t):\n",
    "        mu += y * c / np.sum(c)\n",
    "    r = poisson_chi2(n, mu)\n",
    "    return r\n",
    "\n",
    "cost.errordef = Minuit.LEAST_SQUARES\n",
    "cost.ndata = np.prod(n.shape)\n",
    "\n",
    "starts = np.ones(2)\n",
    "m = Minuit(cost, starts)\n",
    "m.limits = (0, None)\n",
    "m.migrad()\n",
    "m.hesse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The uncertainties reported by the fit correspond to the uncertainty in the data, but not the uncertainty in the templates. The chi2/ndof is also very large, since the uncertainties in the template are not considered in the fit.\n",
    "\n",
    "We bootstrap the templates 1000 times and compute the covariance of the fitted results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 100\n",
    "rng = np.random.default_rng(1)\n",
    "pars = []\n",
    "for ib in range(b):\n",
    "    ti = rng.poisson(t)\n",
    "\n",
    "    def cost(yields):\n",
    "        mu = 0\n",
    "        for y, c in zip(yields, ti):\n",
    "            mu += y * c / np.sum(c)\n",
    "        r = poisson_chi2(n, mu)\n",
    "        return r\n",
    "    \n",
    "    mi = Minuit(cost, m.values[:])\n",
    "    mi.errordef = Minuit.LEAST_SQUARES\n",
    "    mi.limits = (0, None)\n",
    "    mi.strategy = 0\n",
    "    mi.migrad()\n",
    "    assert mi.valid\n",
    "    pars.append(mi.values[:])\n",
    "\n",
    "cov2 = np.cov(np.transpose(pars), ddof=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print the uncertainties from the different stages and the correlation between the two yields.\n",
    "\n",
    "To obtain the full error, we add the independent covariance matrices from the original fit and the bootstrap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov1 = m.covariance\n",
    "\n",
    "for title, cov in zip((\"fit\", \"bootstrap\", \"fit+bootstrap\"), \n",
    "                      (cov1, cov2, cov1 + cov2)):\n",
    "    print(title)\n",
    "    for label, p, e in zip((\"b\", \"s\"), m.values, np.diag(cov) ** 0.5):\n",
    "        print(f\"  {label} {p:.0f} +- {e:.0f}\")\n",
    "    print(f\"  correlation {cov[0, 1] / np.prod(np.diag(cov)) ** 0.5:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bootstrapped template errors are much larger than the fit errors in this case, since the sample used to generate the templates is much smaller than the data sample.\n",
    "\n",
    "The bootstrapped errors for both yields are nearly equal (they become exactly equal if the template sample is large) and the correlation is close to -1 (and becomes exactly -1 in large samples). This is expected, since the data sample is fixed in each iteration. Under these conditions, a change in the templates can only increase the yield of one component at an equal loss for the other component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Template fit with nuisance parameters\n",
    "\n",
    "As described in [Barlow and Beeston, Comput.Phys.Commun. 77 (1993) 219-228](https://doi.org/10.1016/0010-4655(93)90005-W), the correct treatment from first principles is to write down the likelihood function for this case, in which the observed values and unknown parameters are clearly stated. The insight is that the true contents of the bins for the templates are unknown and we need to introduce a nuisance parameter for each bin entry in the template. The combined likelihood for the problem is then combines the estimation of the template yields with the estimation of unknown templates.\n",
    "\n",
    "This problem can be handled straight-forwardly with Minuit, but it leads to the introduction of a large number of nuisance parameters, one for each entry in each template. We again write a cost function for this case (here a class for convenience).\n",
    "\n",
    "As a technical detail, it is necessary to increase the call limit in Migrad for the fit to fully converge, since the limit set by Minuit's default heuristic is too tight for this application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BB:\n",
    "    def __init__(self, n, t):\n",
    "        self.data = n, t\n",
    "\n",
    "    def __call__(self, par):\n",
    "        n, t = self.data\n",
    "        bins = len(n)\n",
    "        yields = par[:2]\n",
    "        nuisances = par[2:]\n",
    "        b = nuisances[:bins]\n",
    "        s = nuisances[bins:]\n",
    "        mu = 0\n",
    "        for y, c in zip(yields, (b, s)):\n",
    "            mu += y * c / np.sum(c)\n",
    "        r = poisson_chi2(n, mu) + poisson_chi2(t[0], b) + poisson_chi2(t[1], s)\n",
    "        return r\n",
    "\n",
    "    @property\n",
    "    def ndata(self):\n",
    "        n, t = self.data\n",
    "        return np.prod(n.shape) + np.prod(t.shape)\n",
    "\n",
    "m1 = Minuit(BB(n, t), np.concatenate([truth, t[0], t[1]]))\n",
    "m1.limits = (0, None)\n",
    "m1.migrad(ncall=100000)\n",
    "m1.hesse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of this fit is comparable to the bootstrap method for this example, but the chi2/ndof is now reasonable and the uncertainties are correct without further work. This method should perform better than the bootstrap method, if the count per bin in the templates is small.\n",
    "\n",
    "Another advantage is of this technique is that one can profile over the likelihood to obtain a 2D confidence regions, which is not possible with the bootstrap technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1.draw_mncontour(\"x0\", \"x1\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on, we briefly explore a possible refinement of the previous method, which is to hide the nuisance parameters from Minuit with a nested fit. It turns out that this technique is not an improvement, but it is useful to show that explicitly.\n",
    "\n",
    "The idea is to construct an outer cost function, which only has the yields as parameters. Inside the outer cost function, the best nuisance parameters are found for the current yields with an inner cost function. Technically, this is achieved by calling a minimizer on the inner cost function at every call to the outer cost function.\n",
    "\n",
    "Technical detail: It is important here to adjust Minuit's expectation of how accurate the cost function is computed. Usually, Minuit performs its internal calculations under the assumption that the cost function is accurate to machine precision. This is usually not the case when a minimizer is used internally to optimize the inner function. We perform the internal minimization with SciPy, which allows us to set the tolerance. We set it here to 1e-8, which is sufficient for this problem and saves a bit of time on the internal minimisation. We then instruct Minuit to expect only this precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = 1e-8\n",
    "\n",
    "def cost(yields):\n",
    "    bins = len(n)\n",
    "\n",
    "    def inner(nuisance):\n",
    "        b = nuisance[:bins]\n",
    "        s = nuisance[bins:]\n",
    "        mu = 0\n",
    "        for y, c in zip(yields, (b, s)):\n",
    "            mu += y * c / np.sum(c)\n",
    "        r = poisson_chi2(n, mu) + poisson_chi2(t[0], b) + poisson_chi2(t[1], s)\n",
    "        return r\n",
    "\n",
    "    bounds = np.zeros((2 * bins, 2))\n",
    "    bounds[:, 1] = np.inf\n",
    "    r = minimize(inner, np.ravel(t), bounds=bounds, tol=precision)\n",
    "    assert r.success\n",
    "    return r.fun\n",
    "\n",
    "cost.errordef = Minuit.LEAST_SQUARES\n",
    "cost.ndata = np.prod(n.shape)\n",
    "\n",
    "m2 = Minuit(cost, truth)\n",
    "m2.precision = precision\n",
    "m2.limits = (0, None)\n",
    "m2.migrad()\n",
    "m2.hesse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the exact same result as expected, but the runtime is much longer (more than a factor 10), which disfavors this technique compared to the straight-forward fit. The minimization is not as efficient, because Minuit cannot exploit correlations between the internal and the external parameters that allow it to converge it faster when it sees all parameters at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Template fits\n",
    "\n",
    "The implementation described by [Barlow and Beeston, Comput.Phys.Commun. 77 (1993) 219-228](https://doi.org/10.1016/0010-4655(93)90005-W) solves the problem similarly to the nested fit described above, but the solution to the inner problem is found with a more efficient algorithm. Unfortunately, their approach still requires numerically solving a non-linear equation per bin. The finite accuracy of the non-linear solver introduces discontinuities in the log-likelihood that confuse Minuit, as noted by [Conway, PHYSTAT 2011, https://arxiv.org/abs/1103.0354](https://doi.org/10.48550/arXiv.1103.0354). To address this, Conway proposes a simplified treatment where the uncertainty in the template is described by a multiplicative factor constrained by a Gaussian. With this simplification, the optimal nuisance parameters can be found by bin-by-bin by solving a quadratic equation which has only one allowed solution that can be found analytically. Conway's method and two other approximate methods are implemented in the built-in Template cost function (see documentation for details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Template(n, xe, t, method=\"jsc\")\n",
    "m3 = Minuit(c, *truth)\n",
    "m3.limits = (0, None)\n",
    "m3.migrad()\n",
    "m3.hesse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Template(n, xe, t, method=\"da\")\n",
    "m4 = Minuit(c, *truth)\n",
    "m4.limits = (0, None)\n",
    "m4.migrad()\n",
    "m4.hesse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Template(n, xe, t, method=\"asy\")\n",
    "m5 = Minuit(c, *truth)\n",
    "m5.limits = (0, None)\n",
    "m5.migrad()\n",
    "m5.hesse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for title, m in zip((\"full fit\", \"T(JSC)\", \"T(DA)\", \"T(ASY)\"), (m1, m3, m4, m5)):\n",
    "    print(title)\n",
    "    cov = m.covariance\n",
    "    for label, p, e in zip((\"x0\", \"x1\"), m.values, np.diag(cov) ** 0.5):\n",
    "        print(f\"  {label} {p:.0f} +- {e:.0f}\")\n",
    "    print(f\"  correlation {cov[0, 1] / (cov[0, 0] * cov[1, 1]) ** 0.5:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best yields found by the Barlow-Beeston-Lite (BBL) methods differ from those found with the Barlow-Beeston (BB) method, because the likelihoods are rather different. In this particular case, the uncertainty for the signal estimated by the BBL is larger. The difference shows up in particular in the 68 % confidence regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = m1.mncontour(\"x0\", \"x1\")\n",
    "c3 = m3.mncontour(\"x0\", \"x1\")\n",
    "c4 = m4.mncontour(\"x0\", \"x1\")\n",
    "c5 = m5.mncontour(\"x0\", \"x1\")\n",
    "plt.plot(c1[:,0], c1[:, 1], label=\"BB\")\n",
    "plt.plot(c3[:,0], c3[:, 1], label=\"TEM(JSC)\")\n",
    "plt.plot(c4[:,0], c4[:, 1], label=\"TEM(DA)\")\n",
    "plt.plot(c5[:,0], c4[:, 1], label=\"TEM(ASY)\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias of the estimate and the estimated variance\n",
    "\n",
    "A single toy experiment cannot be used to judge the performance of these methods. We need to study the properties of these fits applied to sets of toy experiments, which allow us to measure the bias of the estimate itself and the bias of its variance estimate, which should reflect the true variance.\n",
    "\n",
    "We run three sets of experiments, with increasing number of events sampled for the templates. We expect that all methods converge as the sample used to compute the templates grows, since this reduces their relative uncertainties. To judge the estimates, we compute the pull distribution of the estimated signal yield, where the pull is defined as\n",
    "$$\n",
    "z = (\\hat s  - s)/ \\hat V_s^{1/2},\n",
    "$$\n",
    "with true signal yield $s$, estimate $\\hat s$, and the estimated variance $\\hat V_s$ of $\\hat s$. The performance of the method is indicated by the degree of agreement of the mean of $z$ with 0 and the standard deviation with 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@delayed\n",
    "def compute(seed, truth, nmc):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    xe, n, t = generate(rng, nmc, truth, 15)\n",
    "    accept = True\n",
    "    result = []\n",
    "    ma = False\n",
    "    for ti in t:\n",
    "        ma |= ti > 0\n",
    "    m_bb = Minuit(BB(n[ma], t[:, ma]), np.concatenate([truth, *t[:, ma]]))\n",
    "    m_bbl1 = Minuit(Template(n, xe, t, method=\"jsc\"), *truth)\n",
    "    m_bbl2 = Minuit(Template(n, xe, t, method=\"da\"), *truth)\n",
    "    m_bbl3 = Minuit(Template(n, xe, t, method=\"asy\"), *truth)\n",
    "    for m in (m_bb, m_bbl1, m_bbl2, m_bbl3):\n",
    "        m.limits = (0, None)\n",
    "        # try hard to converge and get correct error estimates\n",
    "        for iter in range(10):\n",
    "            m.strategy = 0\n",
    "            m.migrad(ncall=1000000, iterate=1)\n",
    "            m.hesse()\n",
    "            if m.valid and m.accurate:\n",
    "                break\n",
    "        if not m.valid or not m.accurate: # should never happen\n",
    "            print(cost.__name__)\n",
    "            display(m)\n",
    "        accept &= m.valid\n",
    "        result.append((m.values[0], m.values[1], m.covariance[0, 0], m.covariance[1, 1]))\n",
    "    if accept:\n",
    "        return result\n",
    "    return None\n",
    "\n",
    "results = {}\n",
    "for nmc in (200, 1000, 10000):\n",
    "    r = Parallel(-1, verbose=1)(compute(s, truth, nmc) for s in range(50))\n",
    "    r = [_ for _ in r if _ is not None]\n",
    "    results[nmc] = np.array(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nmc, r in results.items():\n",
    "    plt.figure()\n",
    "    for i, label in enumerate((\"BB\", \"T(JSC)\", \"T(DA)\", \"T(ASY)\")):\n",
    "        s = r[:, i, 1]\n",
    "        vs = r[:, i, 3]\n",
    "        z = (s - truth[1]) / vs ** 0.5\n",
    "        plt.hist(z,\n",
    "            label=(\n",
    "                f\"{label:4} \"\n",
    "                f\"<z>={np.mean(z):5.2f} \"\n",
    "                f\"Ïƒ(z)={np.std(z):.2f}\"),\n",
    "            density=True, alpha=0.5, bins=20, range=(-6, 6))\n",
    "    plt.xlabel(\"pull\")\n",
    "    plt.title(f\"N(data) = {np.sum(truth)} N(mc) = {nmc}\")\n",
    "    plt.ylim(0, 0.7)\n",
    "    plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of the BBL methods is comparable to that of the BB method. In all cases, biases are small and the estimated variance is close to the actual variance. When the experiments are repeated with a high number of toy simulations, we find that the HPD and ASY methods perform slightly better than Conway's method."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bdbf20ff2e92a3ae3002db8b02bd1dd1b287e934c884beb29a73dced9dbd0fa3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
