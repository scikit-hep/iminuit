{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binned vs. unbinned fits\n",
    "\n",
    "We compare binned with unbinned fit applied to a toy example of a gaussian signal peak over exponential background.\n",
    "\n",
    "For one-dimensional data, binned fit are preferred. They are usually considerably faster than an unbinned fit and more numerically stable. For multi-dimensional data, however, an unbinned fit can be faster.\n",
    "\n",
    "It is a common misconception that binned fits are inherently biased. This idea originates from the past when it was common (at least in particle physics) to fit binned data with the least-squares method, which is indeed biased, see [Dembinski, Schmelling, Waldi, *Application of the Iterated Weighted Least-Squares Fit to counting experiments*, NIM A 940 (2019) 135-141](https://doi.org/10.1016/j.nima.2019.05.086). That bias can be completely avoided, however, if the fit uses the maximum-likelihood method and a Poisson distribution to describe the observed bin contents as a function of the predicted ones, and if the model prediction for a bin content is properly computed by integrating over the model density, instead of computing it from the density at the bin center times the bin width. The cost functions `BinnedNLL` and `ExtendedBinnedNLL` from `iminuit.cost` use the correct calculation.\n",
    "\n",
    "So there is no need to worry bias, but some information is lost in the binning process - the densities of events inside each bin. This loss can be made negligible by making the bin width small enough. How small the bins have to be depends on the sensitivity of the model parameter on this particular loss of information. In this tutorial we demonstrate this and also demonstrate the difference in run-time of unbinned and binned fits.\n",
    "\n",
    "**Conclusions:** With only 20 bins, the binned fit reached an accuracy for the signal yield that is comparable to the unbinned fit. With 50 bins, also all shape parameters have uncertainties that are less than 5 % larger than those in the unbinned fit. At the same time, the binned fit is much faster. Even with 200 bins, the binned fit is two orders of magnitude faster than the unbinned fit. In practice, this is a huge difference, 3 seconds vs. 5 minutes.\n",
    "\n",
    "You can try to run this notebook with a data sample contains less points, then the difference will not be as dramatic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba_stats import norm, expon\n",
    "import matplotlib.pyplot as plt\n",
    "from iminuit import Minuit\n",
    "from iminuit.cost import ExtendedUnbinnedNLL, ExtendedBinnedNLL\n",
    "import joblib\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample size, change this to see how the results of the comparison change\n",
    "n = 100_000\n",
    "truth = np.array((1.0, 1.0, 1.0, 0.1, 1.0))\n",
    "\n",
    "rng = np.random.default_rng(1)\n",
    "s = rng.normal(truth[2], truth[3], size=int(n * truth[0]))\n",
    "b = rng.exponential(truth[4], size=int(n * truth[1]))\n",
    "pts = np.append(s, b)\n",
    "pts = pts[(pts > 0) & (pts < 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(c):\n",
    "    m = Minuit(c, s=1, b=1, mu=1, sigma=0.1, tau=1)\n",
    "    m.limits[\"s\", \"b\", \"sigma\", \"tau\"] = (0, None)\n",
    "    m.limits[\"mu\"] = (0, 2)\n",
    "    m.migrad()\n",
    "    assert m.valid\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def density(x, s, b, mu, sigma, tau):\n",
    "    xrange = (0, 2)\n",
    "    s1 = s * n * np.diff(norm.cdf(xrange, mu, sigma))\n",
    "    b1 = b * n * np.diff(expon.cdf(xrange, 0, tau))\n",
    "    return s1 + b1, (\n",
    "        s * n * norm.pdf(x, mu, sigma) + \n",
    "        b * n * expon.pdf(x, 0, tau)\n",
    "    )\n",
    "\n",
    "m = fit(ExtendedUnbinnedNLL(pts, density))\n",
    "par_names = [m.params[i].name for i in range(m.npar)]\n",
    "results = {np.inf: (np.array(m.values), np.array(m.errors), m.fmin.time)}\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xm = np.linspace(np.min(pts), np.max(pts), 1000)\n",
    "_, ym = density(xm, *m.values)\n",
    "plt.hist(pts, bins=100, range=(0, 2), label=\"data\")\n",
    "dx = 2 / 100\n",
    "plt.plot(xm, ym * dx, label=\"fit\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"x\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This fit is unbinned, the observed sample is binned here only for visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integral(xe, s, b, mu, sigma, tau):\n",
    "    return s * n * norm.cdf(xe, mu, sigma) + b * n * expon.cdf(xe, 0, tau)\n",
    "\n",
    "fig, ax = plt.subplots(3, 2, figsize=(10, 8), sharex=True, constrained_layout=True)\n",
    "for axi, bins in zip(ax.flat, (200, 100, 50, 20, 10, 5)):\n",
    "    w, xe = np.histogram(pts, bins=bins, range=(0, 2))\n",
    "    c = ExtendedBinnedNLL(w, xe, integral)\n",
    "    m = fit(c)\n",
    "    display(m)\n",
    "    axi.stairs(w, xe, fill=True, label=\"data\")\n",
    "    axi.stairs(np.diff(integral(xe, *m.values)), xe, label=\"fit\")\n",
    "    axi.legend()\n",
    "    results[bins] = (np.array(m.values), np.array(m.errors), m.fmin.time)\n",
    "fig.supxlabel(\"x\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npar = len(results[np.inf][0])\n",
    "\n",
    "fig, ax = plt.subplots(npar, 2, sharex=True, figsize=(14, 20))\n",
    "for j, (k, (v, e, _)) in enumerate(results.items()):\n",
    "    for i, (vi, ei) in enumerate(zip(v, e)):\n",
    "        c = f\"C{i}\"\n",
    "        ax[i, 0].errorbar(j, vi, ei, color=c, fmt=\"o\")\n",
    "        ax[i, 0].set_ylabel(par_names[i])\n",
    "        einf = results[np.inf][1][i]\n",
    "        ax[i, 1].plot(j, ei /einf, \"o\", color=c)\n",
    "for i in range(npar):\n",
    "    ax[i, 1].set_ylim(0.95, 1.2)\n",
    "    ax[i, 1].axhline(1.05, ls=\"--\", color=\"0.5\")\n",
    "plt.xticks(np.arange(7), [f\"{x}\" for x in results.keys()]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shown on the left is the fitted value and its uncertainty estimate. Shown of the right is the relative size of the error bar of the binned fit compared to the unbinned fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "x = np.arange(7)\n",
    "y = [v[2] for v in results.values()]\n",
    "plt.plot(x, y, \"o\")\n",
    "for xi, yi in zip(x[1:], y[1:]):\n",
    "    plt.text(xi, yi * 1.2, f\"{y[0]/yi:.0f}x\", ha=\"center\")\n",
    "plt.xticks(x, [f\"{x}\" for x in results.keys()])\n",
    "plt.ylabel(\"time / sec\")\n",
    "plt.semilogy();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now demonstrate that the binned fits and the unbinned fit are unbiased. We repeat the fit many times with independent random samples, the mean of the results minus the truth is the bias. In each iteration, the binned fits use the same data that the unbinned fit uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@joblib.delayed\n",
    "def run(seed):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    s = rng.normal(truth[2], truth[3], size=int(n * truth[0]))\n",
    "    b = rng.exponential(truth[4], size=int(n * truth[1]))\n",
    "    pts = np.append(s, b)\n",
    "    pts = pts[(pts > 0) & (pts < 2)]\n",
    "\n",
    "    if bins == np.inf:\n",
    "        m = fit(ExtendedUnbinnedNLL(pts, density))\n",
    "        assert m.valid\n",
    "    else:\n",
    "        w, xe = np.histogram(pts, bins=bins, range=(0, 2))\n",
    "        m = fit(ExtendedBinnedNLL(w, xe, integral))\n",
    "        assert m.valid\n",
    "    return np.array(m.values)\n",
    "\n",
    "results = {}\n",
    "for bins in (np.inf, 200, 100, 50, 20, 10, 5):\n",
    "    results[bins] = joblib.Parallel(-1)(run(seed) for seed in range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = None\n",
    "for bin, values in results.items():\n",
    "    plt.figure()\n",
    "    m = np.mean(values, axis=0) - truth\n",
    "    s = np.std(values, axis=0, ddof=1)\n",
    "    plt.title(f\"{bin=}\")\n",
    "    plt.errorbar(np.arange(len(m)), m / s, 1, fmt=\"o\", label=f\"{bin=}\")\n",
    "    plt.axhline(0, ls=\"--\", color=\"0.5\")\n",
    "    plt.xticks(np.arange(len(m)), [\"s\", \"b\", \"mu\", \"sigma\", \"tau\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots show the bias relative to the standard deviation for each parameter. All results are unbiased, whatever the binning. The bias is not exactly zero, since we used only 100 repetitions, it shrinks further with more. One can observe that the residual bias that is coming from the finite sampling is the same for the unbinned fit and the fits with 100 and 200 bins, which are essentially equivalent."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bdbf20ff2e92a3ae3002db8b02bd1dd1b287e934c884beb29a73dced9dbd0fa3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
