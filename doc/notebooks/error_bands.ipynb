{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "frozen-raising",
   "metadata": {},
   "source": [
    "# How to draw error bands\n",
    "\n",
    "We show two ways to compute 1-sigma bands around a fitted curve.\n",
    "\n",
    "Whether the curve describes a probability density (from a maximum-likelihood fit) or an expectation (from a least-squares fit) does not matter, the procedure is the same. We demonstrate this on an unbinned extended maximum-likelihood fit of a Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-dress",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba_stats import norm\n",
    "from iminuit import Minuit\n",
    "from iminuit.cost import ExtendedUnbinnedNLL\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# generate toy sample\n",
    "rng = np.random.default_rng(1)\n",
    "x = rng.normal(size=100)\n",
    "\n",
    "# bin it\n",
    "w, xe = np.histogram(x, bins=100, range=(-5, 5))\n",
    "\n",
    "# compute bin-wise density estimates\n",
    "werr = w ** 0.5\n",
    "cx = 0.5 * (xe[1:] + xe[:-1])\n",
    "dx = np.diff(xe)\n",
    "d = w / dx\n",
    "derr = werr / dx \n",
    "\n",
    "\n",
    "# define model and cost function\n",
    "def model(x, par):\n",
    "    return par[0], par[0] * norm.pdf(x, par[1], par[2])\n",
    "\n",
    "cost = ExtendedUnbinnedNLL(x, model)\n",
    "\n",
    "# fit the model\n",
    "m = Minuit(cost, (1, 0, 1))\n",
    "m.migrad()\n",
    "m.hesse()\n",
    "\n",
    "# plot everything\n",
    "plt.errorbar(cx, d, derr, fmt=\"o\", label=\"data\", zorder=0)\n",
    "\n",
    "plt.plot(cx, model(cx, m.values)[1], lw=3,\n",
    "         label=\"fit\")\n",
    "plt.legend(frameon=False,\n",
    "           title=f\"$n = {m.values[0]:.2f} +/- {m.errors[0]:.2f}$\\n\"\n",
    "                 f\"$\\mu = {m.values[1]:.2f} +/- {m.errors[1]:.2f}$\\n\"\n",
    "                 f\"$\\sigma = {m.values[2]:.2f} +/- {m.errors[2]:.2f}$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "realistic-trail",
   "metadata": {},
   "source": [
    "We want to understand how uncertain the Gaussian curve is. Thus we want to draw a 1-sigma error band around the curve, which approximates the 68 % confidence interval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polyphonic-patient",
   "metadata": {},
   "source": [
    "## With error propagation\n",
    "\n",
    "The uncertainty is quantified in form of the covariance matrix of the fitted parameters. We can use [error propagation](https://en.wikipedia.org/wiki/Propagation_of_uncertainty) to obtain the uncertainty of the curve,\n",
    "\n",
    "$$\n",
    "C' = J \\, C \\, J^T,\n",
    "$$\n",
    "\n",
    "where $C$ is the covariance matrix of the input vector, $C'$ is the covariance matrix of the output vector and $J$ is the matrix of first derivatives of the mapping function between input and output. The mapping in this case is the curve, $\\vec y = f(\\vec{x}; \\vec{p})$, regarded as a function of $\\vec{p}$ and not of $\\vec{x}$, which is fixed. The function maps from $\\vec{p}$ to $\\vec{y}$ and the Jacobi matrix is made from elements\n",
    "\n",
    "$$\n",
    "J_{ik} = \\frac{\\partial y_i}{\\partial p_k}.\n",
    "$$\n",
    "\n",
    "To compute the derivatives one can sometimes use [Sympy](https://www.sympy.org/en/index.html) or an auto-differentiation tool like [JAX](https://jax.readthedocs.io/en/latest/) if the function permits it, but in general they need to be computed numerically. The function `iminuit.util.propagate` does just that and the whole propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technological-justice",
   "metadata": {},
   "outputs": [],
   "source": [
    "from iminuit.util import propagate\n",
    "\n",
    "# run error propagation\n",
    "y, ycov = propagate(lambda p: model(cx, p)[1], m.values, m.covariance)\n",
    "\n",
    "# plot everything\n",
    "plt.errorbar(cx, d, derr, fmt=\"o\", label=\"data\", zorder=0)\n",
    "\n",
    "plt.plot(cx, y, lw=3, label=\"fit\")\n",
    "\n",
    "# draw 1 sigma error band\n",
    "yerr_prop = np.diag(ycov) ** 0.5\n",
    "plt.fill_between(cx, y - yerr_prop, y + yerr_prop, facecolor=\"C1\", alpha=0.5)\n",
    "\n",
    "plt.legend(frameon=False,\n",
    "           title=f\"$n = {m.values[0]:.2f} +/- {m.errors[0]:.2f}$\\n\"\n",
    "                 f\"$\\mu = {m.values[1]:.2f} +/- {m.errors[1]:.2f}$\\n\"\n",
    "                 f\"$\\sigma = {m.values[2]:.2f} +/- {m.errors[2]:.2f}$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-sensitivity",
   "metadata": {},
   "source": [
    "Error propagation is relatively fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continent-astrology",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -r 1 -n 1000\n",
    "propagate(lambda p: model(cx, p)[1], m.values, m.covariance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advance-flight",
   "metadata": {},
   "source": [
    "## With a parametric bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "duplicate-community",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(1)\n",
    "\n",
    "# generate set of bootstrapped parameters\n",
    "par_b = rng.multivariate_normal(m.values, m.covariance, size=1000)\n",
    "\n",
    "# compute bootstrapped curves\n",
    "y_b = [model(cx, p)[1] for p in par_b]\n",
    "yerr_boot = np.std(y_b, axis=0)\n",
    "\n",
    "# plot everything\n",
    "plt.errorbar(cx, d, derr, fmt=\"o\", label=\"data\", zorder=0)\n",
    "\n",
    "plt.plot(cx, y, lw=3, label=\"fit\")\n",
    "\n",
    "# draw 1 sigma error band\n",
    "plt.fill_between(cx, y - yerr_boot, y + yerr_boot, facecolor=\"C1\", alpha=0.5)\n",
    "\n",
    "plt.legend(frameon=False,\n",
    "           title=f\"$n = {m.values[0]:.2f} +/- {m.errors[0]:.2f}$\\n\"\n",
    "                 f\"$\\mu = {m.values[1]:.2f} +/- {m.errors[1]:.2f}$\\n\"\n",
    "                 f\"$\\sigma = {m.values[2]:.2f} +/- {m.errors[2]:.2f}$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greater-surge",
   "metadata": {},
   "source": [
    "The result is visually indistinguishable from before, as it should be. If you worry about deviations between the two methods, read on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excess-parking",
   "metadata": {},
   "source": [
    "Bootstrapping is comparably slow. In this example, computing the band from 1000 samples is an order of magnitude slower than error propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grave-month",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -r 1 -n 100\n",
    "rng = np.random.default_rng(1)\n",
    "par_b = rng.multivariate_normal(m.values, m.covariance, size=1000)\n",
    "y_b = [model(cx, p)[1] for p in par_b]\n",
    "np.std(y_b, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cordless-elder",
   "metadata": {},
   "source": [
    "However, the calculation time scales linearly with the number of samples. One can simply draw fewer samples if the additional uncertainty is acceptable. If we draw only 50 samples, the computational cost is comparable to error propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lucky-happening",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -r 1 -n 1000\n",
    "rng = np.random.default_rng(1)\n",
    "par_b = rng.multivariate_normal(m.values, m.covariance, size=50)\n",
    "y_b = [model(cx, p)[1] for p in par_b]\n",
    "np.std(y_b, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moral-keeping",
   "metadata": {},
   "source": [
    "Let's see how the result looks, whether it deviates noticably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brilliant-football",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute bootstrapped curves with 50 samples\n",
    "par_b = rng.multivariate_normal(m.values, m.covariance, size=50)\n",
    "y_b = [model(cx, p)[1] for p in par_b]\n",
    "yerr_boot_50 = np.std(y_b, axis=0)\n",
    "\n",
    "# plot everything\n",
    "plt.errorbar(cx, d, derr, fmt=\"o\", label=\"data\", zorder=0)\n",
    "\n",
    "plt.plot(cx, y, lw=3, label=\"fit\")\n",
    "\n",
    "# draw 1 sigma error band\n",
    "plt.fill_between(cx, y - yerr_boot_50, y + yerr_boot_50, facecolor=\"C1\", alpha=0.5)\n",
    "\n",
    "plt.legend(frameon=False,\n",
    "           title=f\"$n = {m.values[0]:.2f} +/- {m.errors[0]:.2f}$\\n\"\n",
    "                 f\"$\\mu = {m.values[1]:.2f} +/- {m.errors[1]:.2f}$\\n\"\n",
    "                 f\"$\\sigma = {m.values[2]:.2f} +/- {m.errors[2]:.2f}$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-automation",
   "metadata": {},
   "source": [
    "No, the result is still visually indistinguishable. This suggests that 50 samples can be enough for plotting, which brings the computational cost of the bootstrap on par with error propagation.\n",
    "\n",
    "Numerically, the three error bands differ at the 10 % level in the central region (expected relative error is $50^{-1/2} \\approx 0.14$). The eye cannot pickup these differences, but they are there. The curves differ more in the tails, which is not visible in linear scale, but noticable in log-scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hindu-moderator",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "plt.sca(ax[0])\n",
    "plt.plot(cx, y - yerr_prop, \"-C0\", label=\"prop\")\n",
    "plt.plot(cx, y + yerr_prop, \"-C0\", label=\"prop\")\n",
    "plt.plot(cx, y - yerr_boot, \"--C1\", label=\"boot[1000]\")\n",
    "plt.plot(cx, y + yerr_boot, \"--C1\", label=\"boot[1000]\")\n",
    "plt.plot(cx, y - yerr_boot_50, \":C2\", label=\"boot[50]\")\n",
    "plt.plot(cx, y + yerr_boot_50, \":C2\", label=\"boot[50]\")\n",
    "plt.legend()\n",
    "plt.semilogy();\n",
    "\n",
    "plt.sca(ax[1])\n",
    "plt.plot(cx, yerr_boot / yerr_prop, label=\"boot[1000] / prop\")\n",
    "plt.plot(cx, yerr_boot_50 / yerr_prop, label=\"boot[50] / prop\")\n",
    "plt.legend()\n",
    "plt.axhline(1, ls=\"--\", color=\"0.5\", zorder=0)\n",
    "for delta in (-0.1, 0.1):\n",
    "    plt.axhline(1 + delta, ls=\":\", color=\"0.5\", zorder=0)\n",
    "plt.ylim(0.5, 1.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quiet-watch",
   "metadata": {},
   "source": [
    "We see that the bootstrapped bands are a bit wider in the tails. This is caused by non-linearities that are neglected in error propagation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "following-campaign",
   "metadata": {},
   "source": [
    "## Which is better? Error propagation or bootstrap?\n",
    "\n",
    "There is no clear-cut answer. At the visual level, both methods are usually fine (even with small number of bootstrap samples). Which calculation is more accurate depends on details of the problem. Fortunately, the sources of error are orthogonal for both methods, so each method can be used to check the other.\n",
    "\n",
    "* The bootstrap error is caused by sampling. It can be reduced by drawing more samples, the relative error is proportional to $N^{-1/2}$.\n",
    "* The propagation error is caused by errors in the Jacobian and by the error of using a first-order Taylor series in the computation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
